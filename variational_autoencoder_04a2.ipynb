{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b72d7f9-c215-49f2-b7ba-c576fd2ba019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import skopt\n",
    "from skopt.space import *\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6527a13e-c9ae-420e-b624-8e8e838beaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c743ad44-b72d-419f-93f3-050a3e79a9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab0e0c2-dae3-4f1b-8e0b-982d82374c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd = torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02f3f9-a1a4-487a-95df-ab9ea4c8448d",
   "metadata": {},
   "source": [
    "## Load the interpolated spectrum files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48da9d-e3e7-46dc-9e26-fe740f1107cd",
   "metadata": {},
   "source": [
    "### use this block instead ofthe ones above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f73471-45b7-45f2-896e-f98ea165ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr16qsnr10_dataset_np = np.load('dr16qsnr10_dataset_vae_eval.npy')\n",
    "dr16qsnr10_errterm_np = np.load('dr16qsnr10_errterm_vae_eval.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7ad58-a4a9-42f3-9a7f-e6756af1c75f",
   "metadata": {},
   "source": [
    "normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512b2e73-eeb8-4304-81de-dad060121d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max norms of each spectrum\n",
    "norms = np.max(dr16qsnr10_dataset_np, axis=1)[:,None]\n",
    "# normalize each of the spectra\n",
    "dr16qsnr10_dataset_np = dr16qsnr10_dataset_np/norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7af7e-20c8-445d-b396-af26ca73576d",
   "metadata": {},
   "source": [
    "normalize error terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e330680e-415b-4f35-87cd-c6e99e55e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr16qsnr10_errterm_np = dr16qsnr10_errterm_np/norms # normalize all the flux errors \n",
    "ivar = 1/dr16qsnr10_errterm_np**2 # inverse variance\n",
    "\n",
    "eps = 1./(2e6) # weight cap\n",
    "data_weights = ivar/(1 + eps*ivar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513cfe2b-c0cb-4d66-a1ee-09967d588905",
   "metadata": {},
   "source": [
    "subsampled to 1500 elements each spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3b4d87-f0ac-40cf-8b20-f0ecdbeb9e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21771, 1500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr16qsnr10_dataset_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b1b632-67a4-4aa0-8a5e-558ca410e40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x258e9341e10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dr16qsnr10_dataset_np[245, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad086ed-69d5-497a-9e06-b3be1f2ecf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x258fd3a5d20>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dr16qsnr10_errterm_np[245, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea47e29-fb1e-46fb-bd85-ca3dd4f10426",
   "metadata": {},
   "source": [
    "## VAE implementation\n",
    "Import code file `InfoVAE_2.py` \\\n",
    "adapted from: https://github.com/stephenportillo/SDSS-VAE/blob/master/InfoVAE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae3f707-c012-4594-9e91-e65d0e6999ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd6a15fb-a4cb-4939-aa7a-3f04c8639b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InfoVAE_2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bbcf36-9be0-4c76-ba4a-3ac18ededc77",
   "metadata": {},
   "source": [
    "#### dataset train/valid setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ca94c7-05fe-44ee-b8db-2562868491d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = dr16qsnr10_dataset_np\n",
    "weights = data_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e89ace0f-3229-4792-8c61-3eacdab22871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split - number in each set\n",
    "trainfrac = 0.75\n",
    "ntrain = int(spec.shape[0] * trainfrac)\n",
    "nvalid = spec.shape[0] - ntrain\n",
    "# number of feature (number of pixel in each spectrum)\n",
    "nfeat = spec.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0f38a2f-9e63-4f6c-89b7-3dc9e71dac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "# make validation set deterministic\n",
    "np.random.seed(645847)\n",
    "permutation = np.random.permutation(spec.shape[0])\n",
    "\n",
    "# indices of train set\n",
    "trainidx = permutation[0:ntrain]\n",
    "# validation set indices\n",
    "valididx = permutation[-1-nvalid:-1]\n",
    "# train and test sets\n",
    "trainspec = spec[trainidx,:]\n",
    "validspec = spec[valididx,:]\n",
    "trainweig = weights[trainidx,:]\n",
    "validweig = weights[valididx,:]\n",
    "# np.savez(tag+'/datasplit.npz', trainidx=trainidx, valididx=valididx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ec453b-49a4-4387-ad19-eeeb870320dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16328, 1500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainspec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf2d24d-8a92-43b7-89d3-da6c83b1ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4703b88f-4883-427a-a881-68c1f6e8c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataloader for training in batches\n",
    "# train set\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.tensor(trainspec, dtype=torch.float32), torch.tensor(trainweig, dtype=torch.float32)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "# valid set\n",
    "valdloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.tensor(validspec, dtype=torch.float32), torch.tensor(validweig, dtype=torch.float32)),\n",
    "    batch_size=nvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b2f52-5b93-48a7-9849-6bfe6374a9eb",
   "metadata": {},
   "source": [
    "#### utility setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "073bc5eb-9ea9-46f8-b59c-e3eb4b859613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch): #model, optimizer, epoch, min_valid_loss, badepochs\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_logL = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        spectrum, weig = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, logL, KLD, MMD = model.loss(spectrum, weig, epoch)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        train_logL += logL.item()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(dataloader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_logL = 0\n",
    "        valid_KLD = 0\n",
    "        valid_MMD = 0\n",
    "\n",
    "        for valddata in valdloader:\n",
    "            spectrum, weig = valddata\n",
    "            \n",
    "            loss, logL, KLD, MMD = model.loss(spectrum, weig, epoch)\n",
    "            valid_loss += loss.item()\n",
    "            valid_logL += logL.item()\n",
    "            valid_KLD += KLD.item()\n",
    "            valid_MMD += MMD.item()\n",
    "        \n",
    "        valid_loss /= len(valdloader.dataset)\n",
    "        valid_logL /= -len(valdloader.dataset)\n",
    "        valid_KLD  /= len(valdloader.dataset)\n",
    "        valid_MMD  /= len(valdloader.dataset)\n",
    "    return valid_loss, valid_logL, valid_KLD, valid_MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e120eb1c-b471-4919-823c-7d338c368501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, precision=1e-3, patience=10):\n",
    "        self.precision = precision\n",
    "        self.patience = patience\n",
    "        self.badepochs = 0\n",
    "        self.min_valid_loss = float('inf')\n",
    "        \n",
    "    def step(self, valid_loss):\n",
    "        if valid_loss < self.min_valid_loss*(1-self.precision):\n",
    "            self.badepochs = 0\n",
    "            self.min_valid_loss = valid_loss\n",
    "        else:\n",
    "            self.badepochs += 1\n",
    "        return not (self.badepochs == self.patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020343a-4b3e-4609-ab2b-e702343a289a",
   "metadata": {},
   "source": [
    "#### other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "663ddf37-4e77-4722-8ca8-3c77650e5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of epoches each try\n",
    "epochs = 200\n",
    "# number of tries, each with different configuration\n",
    "n_config = 100\n",
    "# print logs every this many epoches\n",
    "log_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0461f4c7-7c6e-481a-a474-6e08489c79a1",
   "metadata": {},
   "source": [
    "We optimize the hyperparameters using `skopt`'s `gp_minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74703f6c-c00b-4854-8108-5a6fc980cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the space of hyperparameters to search\n",
    "search_space = list()\n",
    "search_space.append(Real(2.0, 2.0 ** 6, 'log-uniform', name='lambda'))\n",
    "# search_space.append(Real(0.0, 0.9, 'uniform', name='alpha'))\n",
    "# search_space.append(Integer(8, 256, 'log-uniform', base=2, name='layer_1'))\n",
    "# search_space.append(Integer(8, 128, 'log-uniform', base=2, name='layer_2'))\n",
    "# search_space.append(Integer(0, 2, 'uniform', name='pre_latent_depth'))\n",
    "# search_space.append(Integer(0, 7, 'uniform', name='pre_latent_width'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae532e23-fbf8-4dc0-a74b-135e719b0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrays to record the results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b92b32e0-c147-48c3-85c5-11248f54b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the folder to store the saved neural networks in\n",
    "tag = 'VAE_dr16qsnr10_hypersearch_2024-12-16d'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27a10a-6abb-4715-bf5b-4bbcdf7f2eb1",
   "metadata": {},
   "source": [
    "#### train with hyperparam optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe44e759-c809-404a-bc76-218d4235c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of latent dimiensions is a hyperparameter independent from others\n",
    "ncode = None\n",
    "# set it to the following values:\n",
    "# ncode_vals = [9, 12, 15]\n",
    "ncode_vals = [3, 6, 12, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a8a1c3b-31f4-46d0-bb8f-ce294ab3428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function used to evaluate a given configuration\n",
    "@use_named_args(search_space)\n",
    "def evaluate_model(**params):\n",
    "    # unpack the hyperparameters\n",
    "    lambd = params['lambda']\n",
    "    # alpha = params['alpha']\n",
    "    alpha = 0.0\n",
    "    nhidden1 = 256\n",
    "    nhidden2 = 128\n",
    "    # pre_depth = params['pre_latent_depth']\n",
    "    pre_depth = 0\n",
    "    # pre_width = params['pre_latent_width']\n",
    "    # the network topology (number of pre-depth layers is 1 hyperparam)\n",
    "    nhidden = [nhidden1, nhidden2] + [ncode,] * pre_depth\n",
    "\n",
    "    # these are not changed\n",
    "    # dropout (not used?)\n",
    "    dropout = 0   #0.9*np.random.uniform()\n",
    "    dfac = 1./(1.-dropout)\n",
    "\n",
    "\t# create model with specific hyperparameters\n",
    "    # print('config: alpha = %0.1f, lambda = %0.1f; hidden layers with %s, nodes' % (alpha, lambd, str(nhidden)))\n",
    "    print('config: alpha = %0.1f, lambda = %0.1f; hidden layers with %s, nodes' % (alpha, lambd, str(nhidden)))\n",
    "\n",
    "    # create model\n",
    "    model = InfoVAE(alpha=alpha, lambd=lambd, nfeat=nfeat, nhidden=nhidden, ncode=ncode, dropout=dropout)\n",
    "\n",
    "    # optimizer and auxilary components for the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5)\n",
    "    stopper = EarlyStopper(patience=10)\n",
    "\n",
    "    # the metric we are optimizing on is validation loss\n",
    "    res_valid_loss = None\n",
    "\n",
    "    # train many epoches\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # train call\n",
    "        valid_loss, valid_logL, valid_KLD, valid_MMD = train(model, optimizer, epoch)\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            print('====> Epoch: {} VALIDATION Loss: {:.2e} logL: {:.2e} KL: {:.2e} MMD: {:.2e}'.format(\n",
    "                  epoch, valid_loss, valid_logL, valid_KLD, valid_MMD))\n",
    "\n",
    "        # scheduler to update weights\n",
    "        scheduler.step(valid_loss)\n",
    "        # early stopper\n",
    "        if (not stopper.step(valid_loss)) or (epoch == epochs):\n",
    "            print('Stopping')\n",
    "            print('====> Epoch: {} VALIDATION Loss: {:.2e} logL: {:.2e} KL: {:.2e} MMD: {:.2e}'.format(\n",
    "                  epoch, valid_loss, valid_logL, valid_KLD, valid_MMD))\n",
    "            model.MSE = -valid_logL\n",
    "            model.KLD = valid_KLD\n",
    "            model.MMD = valid_MMD\n",
    "            # mdl_MSE[i] = model.MSE\n",
    "            # mdl_KLD[i] = model.KLD\n",
    "            # mdl_MMD[i] = model.MMD\n",
    "            \n",
    "            res_valid_loss = float(valid_loss)\n",
    "            \n",
    "            # save model\n",
    "            torch.save(model, tag+'/VAE_%ifeat_lambd-%f_alpha-%f_nhidden-%s.pth' % (ncode, lambd, alpha, '-'.join([str(i) for i in nhidden])))\n",
    "            break\n",
    "\n",
    "\t# minimize valid loss\n",
    "    return res_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a0e7741-4f8a-43a9-9dcf-1adb69ad4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: alpha = 0.0, lambda = 20.5; hidden layers with [256, 128], nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 5 VALIDATION Loss: 1.89e+04 logL: -1.89e+04 KL: 4.95e+00 MMD: 1.39e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.45e+04 KL: 7.07e+00 MMD: 1.39e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.23e+04 logL: -1.22e+04 KL: 8.58e+00 MMD: 1.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.15e+04 logL: -1.15e+04 KL: 9.65e+00 MMD: 1.24e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.04e+04 logL: -1.04e+04 KL: 1.15e+01 MMD: 1.24e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+04 logL: -1.00e+04 KL: 1.30e+01 MMD: 1.10e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 9.81e+03 logL: -9.78e+03 KL: 1.42e+01 MMD: 1.10e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.63e+03 logL: -9.59e+03 KL: 1.54e+01 MMD: 1.02e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 9.52e+03 logL: -9.48e+03 KL: 1.66e+01 MMD: 1.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.45e+03 logL: -9.41e+03 KL: 1.76e+01 MMD: 9.37e-01\n",
      "====> Epoch: 55 VALIDATION Loss: 9.36e+03 logL: -9.32e+03 KL: 1.85e+01 MMD: 1.05e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.97e+03 logL: -8.94e+03 KL: 1.87e+01 MMD: 9.54e-01\n",
      "====> Epoch: 65 VALIDATION Loss: 8.95e+03 logL: -8.92e+03 KL: 1.88e+01 MMD: 8.67e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.95e+03 logL: -8.91e+03 KL: 1.90e+01 MMD: 9.63e-01\n",
      "====> Epoch: 75 VALIDATION Loss: 8.91e+03 logL: -8.87e+03 KL: 1.90e+01 MMD: 8.49e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.91e+03 logL: -8.88e+03 KL: 1.90e+01 MMD: 9.66e-01\n",
      "====> Epoch: 85 VALIDATION Loss: 8.91e+03 logL: -8.87e+03 KL: 1.90e+01 MMD: 9.39e-01\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 8.91e+03 logL: -8.87e+03 KL: 1.90e+01 MMD: 9.39e-01\n",
      "config: alpha = 0.0, lambda = 8.5; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.87e+04 logL: -1.87e+04 KL: 4.89e+00 MMD: 1.26e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+04 logL: -1.46e+04 KL: 6.49e+00 MMD: 1.47e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.26e+04 logL: -1.26e+04 KL: 8.88e+00 MMD: 1.32e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.08e+04 logL: -1.08e+04 KL: 1.07e+01 MMD: 1.32e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.02e+04 logL: -1.02e+04 KL: 1.24e+01 MMD: 1.29e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.82e+03 logL: -9.79e+03 KL: 1.37e+01 MMD: 1.10e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 9.68e+03 logL: -9.66e+03 KL: 1.49e+01 MMD: 1.13e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.79e+03 logL: -9.76e+03 KL: 1.61e+01 MMD: 9.89e-01\n",
      "====> Epoch: 45 VALIDATION Loss: 9.20e+03 logL: -9.18e+03 KL: 1.67e+01 MMD: 1.03e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.16e+03 logL: -9.14e+03 KL: 1.69e+01 MMD: 1.08e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 9.17e+03 logL: -9.14e+03 KL: 1.69e+01 MMD: 9.71e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.16e+03 logL: -9.13e+03 KL: 1.69e+01 MMD: 1.06e+00\n",
      "Stopping\n",
      "====> Epoch: 60 VALIDATION Loss: 9.16e+03 logL: -9.13e+03 KL: 1.69e+01 MMD: 1.06e+00\n",
      "config: alpha = 0.0, lambda = 22.1; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.97e+04 logL: -1.97e+04 KL: 4.24e+00 MMD: 1.02e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+04 logL: -1.46e+04 KL: 6.21e+00 MMD: 1.23e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.25e+04 logL: -1.24e+04 KL: 7.77e+00 MMD: 1.16e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.18e+04 logL: -1.18e+04 KL: 8.70e+00 MMD: 1.13e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.12e+04 logL: -1.12e+04 KL: 1.03e+01 MMD: 1.19e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.04e+04 logL: -1.04e+04 KL: 1.18e+01 MMD: 1.01e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 1.01e+04 logL: -1.01e+04 KL: 1.31e+01 MMD: 1.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.88e+03 logL: -9.84e+03 KL: 1.43e+01 MMD: 1.08e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 9.79e+03 logL: -9.76e+03 KL: 1.53e+01 MMD: 8.78e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.60e+03 logL: -9.56e+03 KL: 1.63e+01 MMD: 9.61e-01\n",
      "====> Epoch: 55 VALIDATION Loss: 9.53e+03 logL: -9.50e+03 KL: 1.72e+01 MMD: 9.68e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.56e+03 logL: -9.52e+03 KL: 1.78e+01 MMD: 1.02e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 9.55e+03 logL: -9.51e+03 KL: 1.82e+01 MMD: 9.89e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 9.19e+03 logL: -9.15e+03 KL: 1.85e+01 MMD: 8.88e-01\n",
      "====> Epoch: 75 VALIDATION Loss: 9.19e+03 logL: -9.16e+03 KL: 1.85e+01 MMD: 7.91e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.16e+03 logL: -9.12e+03 KL: 1.85e+01 MMD: 1.02e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 9.16e+03 logL: -9.12e+03 KL: 1.85e+01 MMD: 8.68e-01\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 9.16e+03 logL: -9.12e+03 KL: 1.85e+01 MMD: 1.03e+00\n",
      "config: alpha = 0.0, lambda = 5.0; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.71e+04 logL: -1.71e+04 KL: 5.14e+00 MMD: 1.50e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.38e+04 logL: -1.38e+04 KL: 6.71e+00 MMD: 1.50e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.22e+04 logL: -1.21e+04 KL: 8.46e+00 MMD: 1.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.10e+04 logL: -1.10e+04 KL: 9.89e+00 MMD: 1.35e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.00e+04 logL: -1.00e+04 KL: 1.18e+01 MMD: 1.26e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.99e+03 logL: -9.97e+03 KL: 1.32e+01 MMD: 8.91e-01\n",
      "====> Epoch: 35 VALIDATION Loss: 9.58e+03 logL: -9.56e+03 KL: 1.45e+01 MMD: 9.42e-01\n",
      "====> Epoch: 40 VALIDATION Loss: 9.51e+03 logL: -9.49e+03 KL: 1.58e+01 MMD: 9.45e-01\n",
      "====> Epoch: 45 VALIDATION Loss: 9.30e+03 logL: -9.28e+03 KL: 1.70e+01 MMD: 8.19e-01\n",
      "====> Epoch: 50 VALIDATION Loss: 9.09e+03 logL: -9.07e+03 KL: 1.81e+01 MMD: 7.89e-01\n",
      "====> Epoch: 55 VALIDATION Loss: 8.98e+03 logL: -8.96e+03 KL: 1.89e+01 MMD: 8.16e-01\n",
      "====> Epoch: 60 VALIDATION Loss: 9.02e+03 logL: -8.99e+03 KL: 1.95e+01 MMD: 8.33e-01\n",
      "====> Epoch: 65 VALIDATION Loss: 8.96e+03 logL: -8.94e+03 KL: 2.00e+01 MMD: 8.38e-01\n",
      "====> Epoch: 70 VALIDATION Loss: 8.90e+03 logL: -8.88e+03 KL: 2.00e+01 MMD: 8.20e-01\n",
      "====> Epoch: 75 VALIDATION Loss: 9.03e+03 logL: -9.00e+03 KL: 1.98e+01 MMD: 9.44e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 8.89e+03 logL: -8.87e+03 KL: 1.95e+01 MMD: 8.33e-01\n",
      "====> Epoch: 85 VALIDATION Loss: 8.55e+03 logL: -8.52e+03 KL: 1.97e+01 MMD: 7.58e-01\n",
      "====> Epoch: 90 VALIDATION Loss: 8.56e+03 logL: -8.54e+03 KL: 1.97e+01 MMD: 9.27e-01\n",
      "====> Epoch: 95 VALIDATION Loss: 8.53e+03 logL: -8.50e+03 KL: 1.97e+01 MMD: 7.85e-01\n",
      "====> Epoch: 100 VALIDATION Loss: 8.52e+03 logL: -8.50e+03 KL: 1.97e+01 MMD: 7.77e-01\n",
      "====> Epoch: 105 VALIDATION Loss: 8.52e+03 logL: -8.50e+03 KL: 1.97e+01 MMD: 8.63e-01\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 8.52e+03 logL: -8.50e+03 KL: 1.97e+01 MMD: 8.63e-01\n",
      "config: alpha = 0.0, lambda = 2.2; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.79e+04 logL: -1.79e+04 KL: 5.02e+00 MMD: 1.34e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.42e+04 KL: 6.86e+00 MMD: 1.53e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.23e+04 logL: -1.23e+04 KL: 8.75e+00 MMD: 1.54e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.16e+04 logL: -1.16e+04 KL: 1.00e+01 MMD: 1.34e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.05e+04 logL: -1.05e+04 KL: 1.22e+01 MMD: 1.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.02e+04 logL: -1.02e+04 KL: 1.38e+01 MMD: 1.26e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 9.91e+03 logL: -9.89e+03 KL: 1.50e+01 MMD: 1.26e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.00e+04 logL: -1.00e+04 KL: 1.62e+01 MMD: 1.20e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 9.76e+03 logL: -9.74e+03 KL: 1.73e+01 MMD: 1.13e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.79e+03 logL: -9.77e+03 KL: 1.82e+01 MMD: 1.03e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 9.32e+03 logL: -9.30e+03 KL: 1.86e+01 MMD: 1.19e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.34e+03 logL: -9.32e+03 KL: 1.87e+01 MMD: 1.02e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 9.31e+03 logL: -9.29e+03 KL: 1.88e+01 MMD: 1.09e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.32e+03 logL: -9.30e+03 KL: 1.89e+01 MMD: 1.09e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 9.28e+03 logL: -9.26e+03 KL: 1.89e+01 MMD: 1.01e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.28e+03 logL: -9.26e+03 KL: 1.89e+01 MMD: 1.12e+00\n",
      "Stopping\n",
      "====> Epoch: 81 VALIDATION Loss: 9.28e+03 logL: -9.26e+03 KL: 1.89e+01 MMD: 1.09e+00\n",
      "config: alpha = 0.0, lambda = 3.4; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.80e+04 logL: -1.80e+04 KL: 4.43e+00 MMD: 1.23e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.45e+04 KL: 5.90e+00 MMD: 1.46e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.22e+04 logL: -1.21e+04 KL: 7.88e+00 MMD: 1.37e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.13e+04 logL: -1.13e+04 KL: 9.34e+00 MMD: 1.45e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.01e+04 logL: -1.01e+04 KL: 1.15e+01 MMD: 1.42e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 9.73e+03 logL: -9.72e+03 KL: 1.31e+01 MMD: 1.36e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 9.44e+03 logL: -9.42e+03 KL: 1.45e+01 MMD: 1.32e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.34e+03 logL: -9.33e+03 KL: 1.56e+01 MMD: 1.02e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 9.27e+03 logL: -9.25e+03 KL: 1.67e+01 MMD: 1.16e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 8.90e+03 logL: -8.88e+03 KL: 1.74e+01 MMD: 1.12e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 8.91e+03 logL: -8.89e+03 KL: 1.76e+01 MMD: 1.12e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 8.90e+03 logL: -8.88e+03 KL: 1.78e+01 MMD: 1.29e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 8.87e+03 logL: -8.85e+03 KL: 1.78e+01 MMD: 1.21e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 8.87e+03 logL: -8.85e+03 KL: 1.78e+01 MMD: 1.12e+00\n",
      "Stopping\n",
      "====> Epoch: 74 VALIDATION Loss: 8.86e+03 logL: -8.84e+03 KL: 1.78e+01 MMD: 1.19e+00\n",
      "config: alpha = 0.0, lambda = 4.1; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.84e+04 logL: -1.83e+04 KL: 4.10e+00 MMD: 1.11e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.48e+04 logL: -1.48e+04 KL: 5.59e+00 MMD: 1.41e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.21e+04 logL: -1.20e+04 KL: 7.62e+00 MMD: 1.37e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.14e+04 logL: -1.14e+04 KL: 9.09e+00 MMD: 1.37e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.06e+04 logL: -1.06e+04 KL: 1.09e+01 MMD: 1.47e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+04 logL: -1.01e+04 KL: 1.29e+01 MMD: 1.33e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 9.72e+03 logL: -9.70e+03 KL: 1.44e+01 MMD: 1.27e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.68e+03 logL: -9.66e+03 KL: 1.56e+01 MMD: 1.20e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 9.69e+03 logL: -9.67e+03 KL: 1.67e+01 MMD: 1.17e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.47e+03 logL: -9.44e+03 KL: 1.77e+01 MMD: 1.23e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 9.42e+03 logL: -9.40e+03 KL: 1.86e+01 MMD: 1.15e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.38e+03 logL: -9.36e+03 KL: 1.92e+01 MMD: 1.34e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 9.34e+03 logL: -9.31e+03 KL: 1.95e+01 MMD: 1.16e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.37e+03 logL: -9.35e+03 KL: 1.95e+01 MMD: 1.20e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 8.98e+03 logL: -8.96e+03 KL: 1.96e+01 MMD: 1.13e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 8.98e+03 logL: -8.95e+03 KL: 1.96e+01 MMD: 1.24e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 8.99e+03 logL: -8.97e+03 KL: 1.96e+01 MMD: 1.24e+00\n",
      "Stopping\n",
      "====> Epoch: 85 VALIDATION Loss: 8.99e+03 logL: -8.97e+03 KL: 1.96e+01 MMD: 1.24e+00\n",
      "config: alpha = 0.0, lambda = 53.9; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.87e+04 logL: -1.86e+04 KL: 4.30e+00 MMD: 1.23e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.52e+04 logL: -1.51e+04 KL: 5.73e+00 MMD: 1.32e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 7.45e+00 MMD: 1.42e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.19e+04 logL: -1.18e+04 KL: 8.80e+00 MMD: 1.42e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.13e+04 logL: -1.12e+04 KL: 1.00e+01 MMD: 1.26e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.07e+04 logL: -1.06e+04 KL: 1.19e+01 MMD: 1.21e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 1.04e+04 logL: -1.04e+04 KL: 1.34e+01 MMD: 1.19e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.95e+03 logL: -9.87e+03 KL: 1.47e+01 MMD: 1.06e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 9.97e+03 logL: -9.90e+03 KL: 1.59e+01 MMD: 1.19e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.79e+03 logL: -9.72e+03 KL: 1.69e+01 MMD: 1.02e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 9.74e+03 logL: -9.67e+03 KL: 1.78e+01 MMD: 1.12e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.53e+03 logL: -9.46e+03 KL: 1.85e+01 MMD: 1.04e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 9.53e+03 logL: -9.45e+03 KL: 1.89e+01 MMD: 1.02e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.19e+03 logL: -9.12e+03 KL: 1.91e+01 MMD: 9.61e-01\n",
      "====> Epoch: 75 VALIDATION Loss: 9.18e+03 logL: -9.12e+03 KL: 1.91e+01 MMD: 8.96e-01\n",
      "====> Epoch: 80 VALIDATION Loss: 9.16e+03 logL: -9.09e+03 KL: 1.91e+01 MMD: 1.08e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 9.15e+03 logL: -9.08e+03 KL: 1.92e+01 MMD: 8.61e-01\n",
      "Stopping\n",
      "====> Epoch: 88 VALIDATION Loss: 9.16e+03 logL: -9.08e+03 KL: 1.92e+01 MMD: 9.95e-01\n",
      "config: alpha = 0.0, lambda = 30.1; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.85e+04 logL: -1.85e+04 KL: 4.32e+00 MMD: 1.09e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.52e+04 logL: -1.52e+04 KL: 5.77e+00 MMD: 1.34e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.27e+04 logL: -1.26e+04 KL: 7.67e+00 MMD: 1.43e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.24e+04 logL: -1.23e+04 KL: 9.17e+00 MMD: 1.45e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.08e+04 logL: -1.07e+04 KL: 1.09e+01 MMD: 1.38e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.01e+04 logL: -1.00e+04 KL: 1.28e+01 MMD: 1.41e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 9.89e+03 logL: -9.84e+03 KL: 1.43e+01 MMD: 1.25e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 9.67e+03 logL: -9.62e+03 KL: 1.56e+01 MMD: 1.24e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 9.91e+03 logL: -9.85e+03 KL: 1.67e+01 MMD: 1.22e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.61e+03 logL: -9.56e+03 KL: 1.78e+01 MMD: 1.27e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 9.56e+03 logL: -9.51e+03 KL: 1.87e+01 MMD: 1.18e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.81e+03 logL: -9.75e+03 KL: 1.94e+01 MMD: 1.35e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 9.54e+03 logL: -9.49e+03 KL: 1.97e+01 MMD: 1.14e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.07e+03 logL: -9.01e+03 KL: 1.98e+01 MMD: 1.29e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 9.06e+03 logL: -9.01e+03 KL: 1.98e+01 MMD: 1.13e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 9.04e+03 logL: -8.98e+03 KL: 1.98e+01 MMD: 1.28e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 9.04e+03 logL: -8.98e+03 KL: 1.98e+01 MMD: 1.25e+00\n",
      "Stopping\n",
      "====> Epoch: 86 VALIDATION Loss: 9.04e+03 logL: -8.98e+03 KL: 1.98e+01 MMD: 1.32e+00\n",
      "config: alpha = 0.0, lambda = 2.7; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.99e+04 logL: -1.99e+04 KL: 3.49e+00 MMD: 1.03e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.56e+04 logL: -1.56e+04 KL: 5.08e+00 MMD: 1.22e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.33e+04 logL: -1.33e+04 KL: 6.64e+00 MMD: 1.25e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.21e+04 logL: -1.21e+04 KL: 8.08e+00 MMD: 1.18e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.18e+04 logL: -1.18e+04 KL: 9.31e+00 MMD: 1.20e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 1.11e+04 logL: -1.11e+04 KL: 1.10e+01 MMD: 1.31e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 1.05e+04 logL: -1.05e+04 KL: 1.27e+01 MMD: 1.22e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 1.02e+04 logL: -1.02e+04 KL: 1.42e+01 MMD: 1.08e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 1.00e+04 logL: -1.00e+04 KL: 1.54e+01 MMD: 1.02e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 9.81e+03 logL: -9.79e+03 KL: 1.66e+01 MMD: 1.06e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 9.78e+03 logL: -9.77e+03 KL: 1.77e+01 MMD: 1.00e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 9.68e+03 logL: -9.65e+03 KL: 1.85e+01 MMD: 1.19e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 9.25e+03 logL: -9.23e+03 KL: 1.87e+01 MMD: 1.09e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 9.22e+03 logL: -9.19e+03 KL: 1.87e+01 MMD: 1.02e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 9.21e+03 logL: -9.19e+03 KL: 1.87e+01 MMD: 9.53e-01\n",
      "Stopping\n",
      "====> Epoch: 79 VALIDATION Loss: 9.21e+03 logL: -9.19e+03 KL: 1.87e+01 MMD: 1.03e+00\n",
      "Best Validation loss: 8519.655\n",
      "Best Parameters: [5.043310624057961]\n",
      "config: alpha = 0.0, lambda = 7.1; hidden layers with [256, 128], nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 5 VALIDATION Loss: 1.77e+04 logL: -1.77e+04 KL: 9.61e+00 MMD: 2.96e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 1.36e+01 MMD: 3.29e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.17e+04 logL: -1.17e+04 KL: 1.58e+01 MMD: 3.30e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.05e+04 logL: -1.05e+04 KL: 1.85e+01 MMD: 3.27e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.96e+03 logL: -8.92e+03 KL: 2.26e+01 MMD: 2.98e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.97e+03 logL: -7.92e+03 KL: 2.64e+01 MMD: 2.91e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.60e+03 logL: -7.55e+03 KL: 2.93e+01 MMD: 2.75e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.46e+03 logL: -7.42e+03 KL: 3.17e+01 MMD: 2.76e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.35e+03 logL: -7.29e+03 KL: 3.38e+01 MMD: 2.84e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.22e+03 logL: -7.17e+03 KL: 3.56e+01 MMD: 2.82e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 7.08e+03 logL: -7.03e+03 KL: 3.68e+01 MMD: 2.54e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.04e+03 logL: -6.99e+03 KL: 3.73e+01 MMD: 2.43e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.76e+03 logL: -6.71e+03 KL: 3.76e+01 MMD: 2.63e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.75e+03 logL: -6.70e+03 KL: 3.77e+01 MMD: 2.76e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.75e+03 logL: -6.69e+03 KL: 3.77e+01 MMD: 2.42e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.72e+03 logL: -6.67e+03 KL: 3.77e+01 MMD: 2.43e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.72e+03 logL: -6.66e+03 KL: 3.77e+01 MMD: 2.46e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.72e+03 logL: -6.66e+03 KL: 3.77e+01 MMD: 2.42e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.72e+03 logL: -6.67e+03 KL: 3.77e+01 MMD: 2.53e+00\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 6.72e+03 logL: -6.67e+03 KL: 3.77e+01 MMD: 2.53e+00\n",
      "config: alpha = 0.0, lambda = 15.7; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.75e+04 logL: -1.74e+04 KL: 8.95e+00 MMD: 2.58e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.39e+04 logL: -1.38e+04 KL: 1.24e+01 MMD: 2.99e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.19e+04 logL: -1.18e+04 KL: 1.50e+01 MMD: 2.95e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.04e+04 logL: -1.03e+04 KL: 1.76e+01 MMD: 2.96e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.08e+03 logL: -9.02e+03 KL: 2.07e+01 MMD: 2.71e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.95e+03 logL: -7.89e+03 KL: 2.42e+01 MMD: 2.34e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.50e+03 logL: -7.43e+03 KL: 2.72e+01 MMD: 2.29e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.42e+03 logL: -7.36e+03 KL: 2.99e+01 MMD: 2.06e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.21e+03 logL: -7.15e+03 KL: 3.21e+01 MMD: 1.92e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.95e+03 logL: -6.89e+03 KL: 3.42e+01 MMD: 1.95e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.95e+03 logL: -6.89e+03 KL: 3.57e+01 MMD: 1.97e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.92e+03 logL: -6.85e+03 KL: 3.66e+01 MMD: 1.88e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.78e+03 logL: -6.72e+03 KL: 3.67e+01 MMD: 1.77e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.80e+03 logL: -6.74e+03 KL: 3.64e+01 MMD: 1.74e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.43e+03 logL: -6.37e+03 KL: 3.63e+01 MMD: 1.77e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.45e+03 logL: -6.38e+03 KL: 3.63e+01 MMD: 1.86e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.41e+03 logL: -6.35e+03 KL: 3.63e+01 MMD: 1.76e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.41e+03 logL: -6.35e+03 KL: 3.63e+01 MMD: 1.89e+00\n",
      "Stopping\n",
      "====> Epoch: 92 VALIDATION Loss: 6.41e+03 logL: -6.35e+03 KL: 3.63e+01 MMD: 1.70e+00\n",
      "config: alpha = 0.0, lambda = 2.3; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.73e+04 logL: -1.73e+04 KL: 6.63e+00 MMD: 2.08e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.45e+04 KL: 9.41e+00 MMD: 2.65e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.19e+04 logL: -1.19e+04 KL: 1.27e+01 MMD: 2.94e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.09e+04 logL: -1.09e+04 KL: 1.59e+01 MMD: 2.71e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.53e+03 logL: -9.50e+03 KL: 1.99e+01 MMD: 2.82e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.39e+03 logL: -8.37e+03 KL: 2.41e+01 MMD: 2.79e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.91e+03 logL: -7.88e+03 KL: 2.78e+01 MMD: 2.66e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.69e+03 logL: -7.66e+03 KL: 3.06e+01 MMD: 2.54e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.31e+03 logL: -7.28e+03 KL: 3.28e+01 MMD: 2.52e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.24e+03 logL: -7.20e+03 KL: 3.48e+01 MMD: 2.61e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 7.07e+03 logL: -7.03e+03 KL: 3.63e+01 MMD: 2.41e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.98e+03 logL: -6.94e+03 KL: 3.70e+01 MMD: 2.37e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.98e+03 logL: -6.94e+03 KL: 3.71e+01 MMD: 2.34e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.68e+03 logL: -6.64e+03 KL: 3.70e+01 MMD: 2.37e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.67e+03 logL: -6.63e+03 KL: 3.70e+01 MMD: 2.40e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.67e+03 logL: -6.63e+03 KL: 3.70e+01 MMD: 2.42e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.66e+03 logL: -6.62e+03 KL: 3.70e+01 MMD: 2.46e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.66e+03 logL: -6.62e+03 KL: 3.69e+01 MMD: 2.60e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.66e+03 logL: -6.62e+03 KL: 3.69e+01 MMD: 2.41e+00\n",
      "Stopping\n",
      "====> Epoch: 95 VALIDATION Loss: 6.66e+03 logL: -6.62e+03 KL: 3.69e+01 MMD: 2.41e+00\n",
      "config: alpha = 0.0, lambda = 8.5; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.98e+04 logL: -1.98e+04 KL: 7.97e+00 MMD: 2.63e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.56e+04 logL: -1.56e+04 KL: 1.13e+01 MMD: 3.18e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.23e+04 logL: -1.23e+04 KL: 1.41e+01 MMD: 3.13e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.16e+04 logL: -1.15e+04 KL: 1.62e+01 MMD: 3.31e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.81e+03 logL: -9.77e+03 KL: 1.98e+01 MMD: 3.15e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.64e+03 logL: -8.59e+03 KL: 2.38e+01 MMD: 3.09e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 8.26e+03 logL: -8.21e+03 KL: 2.71e+01 MMD: 2.85e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.73e+03 logL: -7.68e+03 KL: 2.99e+01 MMD: 2.76e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.53e+03 logL: -7.47e+03 KL: 3.23e+01 MMD: 2.64e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.41e+03 logL: -7.35e+03 KL: 3.44e+01 MMD: 2.69e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 7.33e+03 logL: -7.27e+03 KL: 3.61e+01 MMD: 2.74e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.17e+03 logL: -7.11e+03 KL: 3.72e+01 MMD: 2.52e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 7.12e+03 logL: -7.06e+03 KL: 3.78e+01 MMD: 2.57e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.11e+03 logL: -7.06e+03 KL: 3.79e+01 MMD: 2.47e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.82e+03 logL: -6.77e+03 KL: 3.79e+01 MMD: 2.71e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.85e+03 logL: -6.79e+03 KL: 3.79e+01 MMD: 2.45e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.82e+03 logL: -6.77e+03 KL: 3.79e+01 MMD: 2.41e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.81e+03 logL: -6.75e+03 KL: 3.78e+01 MMD: 2.58e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.80e+03 logL: -6.75e+03 KL: 3.78e+01 MMD: 2.41e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.80e+03 logL: -6.74e+03 KL: 3.77e+01 MMD: 2.51e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 6.82e+03 logL: -6.76e+03 KL: 3.77e+01 MMD: 2.68e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.77e+03 logL: -6.71e+03 KL: 3.77e+01 MMD: 2.52e+00\n",
      "====> Epoch: 115 VALIDATION Loss: 6.77e+03 logL: -6.71e+03 KL: 3.77e+01 MMD: 2.68e+00\n",
      "Stopping\n",
      "====> Epoch: 119 VALIDATION Loss: 6.77e+03 logL: -6.72e+03 KL: 3.77e+01 MMD: 2.34e+00\n",
      "config: alpha = 0.0, lambda = 3.3; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.75e+04 logL: -1.75e+04 KL: 6.65e+00 MMD: 1.92e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.44e+04 KL: 9.33e+00 MMD: 2.59e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.18e+04 logL: -1.18e+04 KL: 1.21e+01 MMD: 2.92e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.14e+04 logL: -1.13e+04 KL: 1.40e+01 MMD: 3.03e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 1.04e+04 logL: -1.04e+04 KL: 1.66e+01 MMD: 3.11e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.70e+03 logL: -8.67e+03 KL: 2.20e+01 MMD: 3.03e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.92e+03 logL: -7.89e+03 KL: 2.60e+01 MMD: 2.74e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.52e+03 logL: -7.49e+03 KL: 2.88e+01 MMD: 2.62e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.42e+03 logL: -7.38e+03 KL: 3.11e+01 MMD: 2.65e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.23e+03 logL: -7.19e+03 KL: 3.30e+01 MMD: 2.57e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 7.15e+03 logL: -7.11e+03 KL: 3.45e+01 MMD: 2.48e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.09e+03 logL: -7.05e+03 KL: 3.57e+01 MMD: 2.31e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.98e+03 logL: -6.94e+03 KL: 3.63e+01 MMD: 2.33e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.91e+03 logL: -6.87e+03 KL: 3.63e+01 MMD: 2.31e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.92e+03 logL: -6.88e+03 KL: 3.61e+01 MMD: 2.58e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.84e+03 logL: -6.80e+03 KL: 3.57e+01 MMD: 2.38e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.62e+03 logL: -6.57e+03 KL: 3.54e+01 MMD: 2.23e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.62e+03 logL: -6.58e+03 KL: 3.54e+01 MMD: 2.07e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.59e+03 logL: -6.55e+03 KL: 3.53e+01 MMD: 2.36e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.60e+03 logL: -6.56e+03 KL: 3.53e+01 MMD: 2.24e+00\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 6.59e+03 logL: -6.55e+03 KL: 3.53e+01 MMD: 2.31e+00\n",
      "config: alpha = 0.0, lambda = 6.1; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.74e+04 logL: -1.74e+04 KL: 7.11e+00 MMD: 2.40e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.31e+04 logL: -1.31e+04 KL: 1.03e+01 MMD: 3.09e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.15e+04 logL: -1.14e+04 KL: 1.32e+01 MMD: 3.24e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.02e+04 logL: -1.01e+04 KL: 1.70e+01 MMD: 3.01e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.67e+03 logL: -8.63e+03 KL: 2.16e+01 MMD: 2.81e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.77e+03 logL: -7.73e+03 KL: 2.56e+01 MMD: 2.72e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.42e+03 logL: -7.38e+03 KL: 2.87e+01 MMD: 2.62e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.20e+03 logL: -7.16e+03 KL: 3.13e+01 MMD: 2.44e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.04e+03 logL: -6.99e+03 KL: 3.36e+01 MMD: 2.48e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.99e+03 logL: -6.94e+03 KL: 3.54e+01 MMD: 2.45e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.79e+03 logL: -6.74e+03 KL: 3.67e+01 MMD: 2.43e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.84e+03 logL: -6.79e+03 KL: 3.72e+01 MMD: 2.43e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 7.01e+03 logL: -6.96e+03 KL: 3.74e+01 MMD: 2.38e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.83e+03 logL: -6.78e+03 KL: 3.70e+01 MMD: 2.33e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.69e+03 logL: -6.64e+03 KL: 3.64e+01 MMD: 2.25e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.72e+03 logL: -6.68e+03 KL: 3.55e+01 MMD: 2.40e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.66e+03 logL: -6.62e+03 KL: 3.49e+01 MMD: 2.40e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.59e+03 logL: -6.55e+03 KL: 3.42e+01 MMD: 2.52e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.35e+03 logL: -6.30e+03 KL: 3.36e+01 MMD: 2.22e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.36e+03 logL: -6.32e+03 KL: 3.36e+01 MMD: 2.40e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 6.33e+03 logL: -6.29e+03 KL: 3.36e+01 MMD: 2.39e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.33e+03 logL: -6.29e+03 KL: 3.36e+01 MMD: 2.27e+00\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 6.33e+03 logL: -6.28e+03 KL: 3.36e+01 MMD: 2.21e+00\n",
      "config: alpha = 0.0, lambda = 23.4; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.68e+04 logL: -1.67e+04 KL: 8.99e+00 MMD: 2.61e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+04 logL: -1.45e+04 KL: 1.06e+01 MMD: 2.83e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.21e+04 logL: -1.20e+04 KL: 1.30e+01 MMD: 2.91e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.13e+04 logL: -1.12e+04 KL: 1.51e+01 MMD: 2.99e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.73e+03 logL: -9.65e+03 KL: 1.89e+01 MMD: 2.93e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.55e+03 logL: -8.46e+03 KL: 2.34e+01 MMD: 2.96e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.80e+03 logL: -7.71e+03 KL: 2.73e+01 MMD: 2.68e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.26e+03 logL: -7.17e+03 KL: 3.01e+01 MMD: 2.72e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.07e+03 logL: -6.98e+03 KL: 3.23e+01 MMD: 2.66e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.98e+03 logL: -6.90e+03 KL: 3.43e+01 MMD: 2.35e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.91e+03 logL: -6.82e+03 KL: 3.57e+01 MMD: 2.46e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.87e+03 logL: -6.78e+03 KL: 3.64e+01 MMD: 2.33e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.76e+03 logL: -6.67e+03 KL: 3.66e+01 MMD: 2.52e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.68e+03 logL: -6.58e+03 KL: 3.61e+01 MMD: 2.60e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.66e+03 logL: -6.58e+03 KL: 3.59e+01 MMD: 2.24e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.46e+03 logL: -6.38e+03 KL: 3.56e+01 MMD: 2.30e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.46e+03 logL: -6.37e+03 KL: 3.55e+01 MMD: 2.26e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.45e+03 logL: -6.36e+03 KL: 3.55e+01 MMD: 2.43e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.44e+03 logL: -6.35e+03 KL: 3.55e+01 MMD: 2.40e+00\n",
      "Stopping\n",
      "====> Epoch: 98 VALIDATION Loss: 6.44e+03 logL: -6.35e+03 KL: 3.55e+01 MMD: 2.34e+00\n",
      "config: alpha = 0.0, lambda = 32.9; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.71e+04 logL: -1.70e+04 KL: 1.07e+01 MMD: 2.60e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.32e+04 logL: -1.31e+04 KL: 1.50e+01 MMD: 2.98e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.19e+04 logL: -1.18e+04 KL: 1.73e+01 MMD: 3.18e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.11e+04 logL: -1.10e+04 KL: 1.91e+01 MMD: 3.15e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.23e+03 logL: -9.11e+03 KL: 2.30e+01 MMD: 3.13e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.49e+03 logL: -8.37e+03 KL: 2.66e+01 MMD: 2.83e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.77e+03 logL: -7.66e+03 KL: 2.98e+01 MMD: 2.47e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.48e+03 logL: -7.36e+03 KL: 3.26e+01 MMD: 2.60e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.21e+03 logL: -7.10e+03 KL: 3.49e+01 MMD: 2.45e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.10e+03 logL: -6.99e+03 KL: 3.69e+01 MMD: 2.31e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.93e+03 logL: -6.82e+03 KL: 3.83e+01 MMD: 2.23e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.89e+03 logL: -6.78e+03 KL: 3.91e+01 MMD: 2.10e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.81e+03 logL: -6.70e+03 KL: 3.92e+01 MMD: 2.22e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.78e+03 logL: -6.67e+03 KL: 3.85e+01 MMD: 2.19e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.78e+03 logL: -6.68e+03 KL: 3.78e+01 MMD: 1.92e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.81e+03 logL: -6.71e+03 KL: 3.69e+01 MMD: 2.06e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.45e+03 logL: -6.34e+03 KL: 3.62e+01 MMD: 2.18e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.43e+03 logL: -6.33e+03 KL: 3.61e+01 MMD: 2.11e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.44e+03 logL: -6.33e+03 KL: 3.60e+01 MMD: 2.13e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.43e+03 logL: -6.33e+03 KL: 3.59e+01 MMD: 2.03e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 6.42e+03 logL: -6.32e+03 KL: 3.58e+01 MMD: 2.05e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.44e+03 logL: -6.34e+03 KL: 3.57e+01 MMD: 2.00e+00\n",
      "====> Epoch: 115 VALIDATION Loss: 6.41e+03 logL: -6.30e+03 KL: 3.56e+01 MMD: 2.09e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.41e+03 logL: -6.30e+03 KL: 3.56e+01 MMD: 2.31e+00\n",
      "Stopping\n",
      "====> Epoch: 124 VALIDATION Loss: 6.40e+03 logL: -6.30e+03 KL: 3.56e+01 MMD: 2.17e+00\n",
      "config: alpha = 0.0, lambda = 60.7; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.90e+04 logL: -1.89e+04 KL: 7.10e+00 MMD: 2.10e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.40e+04 KL: 9.97e+00 MMD: 2.30e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.27e+04 logL: -1.26e+04 KL: 1.20e+01 MMD: 2.29e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.16e+04 logL: -1.14e+04 KL: 1.38e+01 MMD: 2.48e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.70e+03 logL: -9.55e+03 KL: 1.78e+01 MMD: 2.14e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.62e+03 logL: -8.49e+03 KL: 2.19e+01 MMD: 1.71e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.94e+03 logL: -7.81e+03 KL: 2.55e+01 MMD: 1.81e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.47e+03 logL: -7.34e+03 KL: 2.85e+01 MMD: 1.64e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.40e+03 logL: -7.27e+03 KL: 3.09e+01 MMD: 1.72e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.09e+03 logL: -6.96e+03 KL: 3.31e+01 MMD: 1.58e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 7.13e+03 logL: -7.01e+03 KL: 3.49e+01 MMD: 1.43e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.98e+03 logL: -6.85e+03 KL: 3.61e+01 MMD: 1.67e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.91e+03 logL: -6.78e+03 KL: 3.67e+01 MMD: 1.58e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.92e+03 logL: -6.80e+03 KL: 3.69e+01 MMD: 1.33e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.58e+03 logL: -6.45e+03 KL: 3.69e+01 MMD: 1.56e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.57e+03 logL: -6.45e+03 KL: 3.69e+01 MMD: 1.44e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.55e+03 logL: -6.42e+03 KL: 3.68e+01 MMD: 1.50e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.54e+03 logL: -6.42e+03 KL: 3.68e+01 MMD: 1.46e+00\n",
      "Stopping\n",
      "====> Epoch: 93 VALIDATION Loss: 6.55e+03 logL: -6.43e+03 KL: 3.68e+01 MMD: 1.42e+00\n",
      "config: alpha = 0.0, lambda = 11.7; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.86e+04 logL: -1.85e+04 KL: 7.10e+00 MMD: 2.09e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.46e+04 logL: -1.46e+04 KL: 1.01e+01 MMD: 2.43e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.23e+04 logL: -1.23e+04 KL: 1.25e+01 MMD: 2.34e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.09e+04 logL: -1.08e+04 KL: 1.49e+01 MMD: 2.17e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.18e+03 logL: -9.14e+03 KL: 1.89e+01 MMD: 2.18e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.53e+03 logL: -8.48e+03 KL: 2.28e+01 MMD: 2.04e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.87e+03 logL: -7.82e+03 KL: 2.61e+01 MMD: 1.98e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.72e+03 logL: -7.67e+03 KL: 2.88e+01 MMD: 1.87e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 7.40e+03 logL: -7.35e+03 KL: 3.12e+01 MMD: 1.73e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 7.18e+03 logL: -7.13e+03 KL: 3.32e+01 MMD: 1.68e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 7.15e+03 logL: -7.09e+03 KL: 3.49e+01 MMD: 1.86e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 7.08e+03 logL: -7.03e+03 KL: 3.57e+01 MMD: 1.69e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 7.33e+03 logL: -7.27e+03 KL: 3.64e+01 MMD: 1.72e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 7.08e+03 logL: -7.03e+03 KL: 3.60e+01 MMD: 1.64e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.89e+03 logL: -6.84e+03 KL: 3.59e+01 MMD: 1.69e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.98e+03 logL: -6.92e+03 KL: 3.54e+01 MMD: 1.58e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.61e+03 logL: -6.56e+03 KL: 3.50e+01 MMD: 1.70e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.59e+03 logL: -6.54e+03 KL: 3.50e+01 MMD: 1.76e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.59e+03 logL: -6.54e+03 KL: 3.50e+01 MMD: 1.61e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.57e+03 logL: -6.52e+03 KL: 3.49e+01 MMD: 1.66e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 6.57e+03 logL: -6.52e+03 KL: 3.49e+01 MMD: 1.73e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.57e+03 logL: -6.52e+03 KL: 3.49e+01 MMD: 1.55e+00\n",
      "Stopping\n",
      "====> Epoch: 110 VALIDATION Loss: 6.57e+03 logL: -6.52e+03 KL: 3.49e+01 MMD: 1.55e+00\n",
      "Best Validation loss: 6327.255\n",
      "Best Parameters: [6.059668283296925]\n",
      "config: alpha = 0.0, lambda = 3.2; hidden layers with [256, 128], nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 5 VALIDATION Loss: 1.74e+04 logL: -1.74e+04 KL: 1.47e+01 MMD: 4.88e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.34e+04 logL: -1.34e+04 KL: 2.08e+01 MMD: 6.01e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.21e+04 logL: -1.21e+04 KL: 2.54e+01 MMD: 6.65e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.06e+04 logL: -1.06e+04 KL: 3.15e+01 MMD: 6.52e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.10e+03 logL: -9.05e+03 KL: 3.88e+01 MMD: 6.49e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.80e+03 logL: -7.74e+03 KL: 4.66e+01 MMD: 5.94e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.24e+03 logL: -7.17e+03 KL: 5.40e+01 MMD: 5.68e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.91e+03 logL: -6.84e+03 KL: 5.99e+01 MMD: 5.49e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.59e+03 logL: -6.51e+03 KL: 6.50e+01 MMD: 5.40e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.55e+03 logL: -6.47e+03 KL: 6.91e+01 MMD: 5.20e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.50e+03 logL: -6.42e+03 KL: 7.15e+01 MMD: 4.97e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.35e+03 logL: -6.27e+03 KL: 7.30e+01 MMD: 5.52e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.30e+03 logL: -6.22e+03 KL: 7.26e+01 MMD: 5.01e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.27e+03 logL: -6.19e+03 KL: 7.15e+01 MMD: 5.33e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.40e+03 logL: -6.32e+03 KL: 7.02e+01 MMD: 5.03e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.97e+03 logL: -5.89e+03 KL: 6.89e+01 MMD: 5.09e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.96e+03 logL: -5.88e+03 KL: 6.87e+01 MMD: 4.98e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.94e+03 logL: -5.87e+03 KL: 6.86e+01 MMD: 4.98e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.95e+03 logL: -5.87e+03 KL: 6.85e+01 MMD: 5.23e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.94e+03 logL: -5.86e+03 KL: 6.85e+01 MMD: 5.30e+00\n",
      "Stopping\n",
      "====> Epoch: 100 VALIDATION Loss: 5.94e+03 logL: -5.86e+03 KL: 6.85e+01 MMD: 5.30e+00\n",
      "config: alpha = 0.0, lambda = 6.0; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.78e+04 logL: -1.77e+04 KL: 1.23e+01 MMD: 4.38e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.36e+04 logL: -1.35e+04 KL: 1.75e+01 MMD: 5.21e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.15e+04 logL: -1.14e+04 KL: 2.21e+01 MMD: 5.85e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.04e+04 logL: -1.04e+04 KL: 2.65e+01 MMD: 5.77e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.97e+03 logL: -8.90e+03 KL: 3.33e+01 MMD: 5.92e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.77e+03 logL: -7.70e+03 KL: 4.12e+01 MMD: 5.44e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.12e+03 logL: -7.04e+03 KL: 4.80e+01 MMD: 5.24e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.83e+03 logL: -6.75e+03 KL: 5.41e+01 MMD: 5.25e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.57e+03 logL: -6.49e+03 KL: 5.92e+01 MMD: 4.98e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.42e+03 logL: -6.34e+03 KL: 6.36e+01 MMD: 4.84e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.42e+03 logL: -6.33e+03 KL: 6.68e+01 MMD: 4.88e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.23e+03 logL: -6.14e+03 KL: 6.85e+01 MMD: 4.87e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.20e+03 logL: -6.10e+03 KL: 6.93e+01 MMD: 4.55e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.18e+03 logL: -6.09e+03 KL: 6.89e+01 MMD: 4.65e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 5.93e+03 logL: -5.83e+03 KL: 6.88e+01 MMD: 4.71e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.92e+03 logL: -5.83e+03 KL: 6.88e+01 MMD: 4.60e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.92e+03 logL: -5.83e+03 KL: 6.87e+01 MMD: 4.58e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.90e+03 logL: -5.80e+03 KL: 6.87e+01 MMD: 4.82e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.90e+03 logL: -5.81e+03 KL: 6.87e+01 MMD: 4.74e+00\n",
      "Stopping\n",
      "====> Epoch: 96 VALIDATION Loss: 5.90e+03 logL: -5.81e+03 KL: 6.87e+01 MMD: 4.48e+00\n",
      "config: alpha = 0.0, lambda = 2.8; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.84e+04 logL: -1.84e+04 KL: 1.26e+01 MMD: 4.43e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.52e+04 logL: -1.52e+04 KL: 1.83e+01 MMD: 5.38e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.24e+04 logL: -1.23e+04 KL: 2.25e+01 MMD: 5.98e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.09e+04 logL: -1.09e+04 KL: 2.73e+01 MMD: 6.28e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.60e+03 logL: -9.55e+03 KL: 3.43e+01 MMD: 6.16e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.28e+03 logL: -8.23e+03 KL: 4.26e+01 MMD: 5.82e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.56e+03 logL: -7.50e+03 KL: 4.96e+01 MMD: 5.72e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.03e+03 logL: -6.97e+03 KL: 5.55e+01 MMD: 5.51e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.85e+03 logL: -6.78e+03 KL: 6.06e+01 MMD: 5.13e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.68e+03 logL: -6.61e+03 KL: 6.47e+01 MMD: 5.07e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.62e+03 logL: -6.54e+03 KL: 6.77e+01 MMD: 4.98e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.63e+03 logL: -6.55e+03 KL: 6.95e+01 MMD: 4.70e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.22e+03 logL: -6.14e+03 KL: 6.99e+01 MMD: 4.99e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.22e+03 logL: -6.14e+03 KL: 7.00e+01 MMD: 4.85e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.21e+03 logL: -6.13e+03 KL: 7.01e+01 MMD: 5.35e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.20e+03 logL: -6.13e+03 KL: 7.01e+01 MMD: 5.07e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.20e+03 logL: -6.12e+03 KL: 7.02e+01 MMD: 5.05e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.19e+03 logL: -6.11e+03 KL: 7.03e+01 MMD: 4.94e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.18e+03 logL: -6.10e+03 KL: 7.03e+01 MMD: 4.93e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.18e+03 logL: -6.10e+03 KL: 7.03e+01 MMD: 5.06e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 6.16e+03 logL: -6.08e+03 KL: 7.03e+01 MMD: 4.81e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.16e+03 logL: -6.08e+03 KL: 7.03e+01 MMD: 5.02e+00\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 6.15e+03 logL: -6.07e+03 KL: 7.03e+01 MMD: 4.91e+00\n",
      "config: alpha = 0.0, lambda = 8.7; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.81e+04 logL: -1.81e+04 KL: 1.15e+01 MMD: 3.66e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.41e+04 KL: 1.70e+01 MMD: 4.88e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.22e+04 logL: -1.21e+04 KL: 2.13e+01 MMD: 5.51e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.11e+04 logL: -1.10e+04 KL: 2.54e+01 MMD: 5.57e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.27e+03 logL: -9.19e+03 KL: 3.23e+01 MMD: 5.53e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.03e+03 logL: -7.95e+03 KL: 3.98e+01 MMD: 5.28e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.37e+03 logL: -7.28e+03 KL: 4.63e+01 MMD: 5.18e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.09e+03 logL: -7.00e+03 KL: 5.19e+01 MMD: 5.12e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.78e+03 logL: -6.68e+03 KL: 5.68e+01 MMD: 4.70e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.60e+03 logL: -6.51e+03 KL: 6.10e+01 MMD: 4.79e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.44e+03 logL: -6.35e+03 KL: 6.41e+01 MMD: 4.51e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.39e+03 logL: -6.29e+03 KL: 6.60e+01 MMD: 4.87e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.34e+03 logL: -6.23e+03 KL: 6.64e+01 MMD: 4.57e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.43e+03 logL: -6.33e+03 KL: 6.57e+01 MMD: 4.31e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.24e+03 logL: -6.14e+03 KL: 6.50e+01 MMD: 4.32e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.26e+03 logL: -6.16e+03 KL: 6.37e+01 MMD: 4.64e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.26e+03 logL: -6.16e+03 KL: 6.28e+01 MMD: 4.48e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.17e+03 logL: -6.07e+03 KL: 6.20e+01 MMD: 4.53e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.17e+03 logL: -6.07e+03 KL: 6.14e+01 MMD: 4.56e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.09e+03 logL: -6.00e+03 KL: 6.06e+01 MMD: 4.38e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 6.13e+03 logL: -6.03e+03 KL: 6.02e+01 MMD: 4.46e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.92e+03 logL: -5.83e+03 KL: 6.01e+01 MMD: 4.61e+00\n",
      "====> Epoch: 115 VALIDATION Loss: 5.91e+03 logL: -5.81e+03 KL: 6.01e+01 MMD: 4.50e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.90e+03 logL: -5.81e+03 KL: 6.01e+01 MMD: 4.23e+00\n",
      "====> Epoch: 125 VALIDATION Loss: 5.88e+03 logL: -5.79e+03 KL: 6.01e+01 MMD: 4.28e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.89e+03 logL: -5.79e+03 KL: 6.00e+01 MMD: 4.55e+00\n",
      "====> Epoch: 135 VALIDATION Loss: 5.88e+03 logL: -5.79e+03 KL: 6.00e+01 MMD: 4.26e+00\n",
      "Stopping\n",
      "====> Epoch: 137 VALIDATION Loss: 5.89e+03 logL: -5.79e+03 KL: 6.00e+01 MMD: 4.81e+00\n",
      "config: alpha = 0.0, lambda = 26.0; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.87e+04 logL: -1.86e+04 KL: 1.11e+01 MMD: 3.74e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.42e+04 logL: -1.41e+04 KL: 1.55e+01 MMD: 4.52e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.20e+04 logL: -1.18e+04 KL: 1.90e+01 MMD: 5.35e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.12e+04 logL: -1.11e+04 KL: 2.19e+01 MMD: 5.37e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.16e+03 logL: -9.00e+03 KL: 3.00e+01 MMD: 5.18e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.92e+03 logL: -7.76e+03 KL: 3.80e+01 MMD: 5.08e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.27e+03 logL: -7.10e+03 KL: 4.52e+01 MMD: 4.92e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.75e+03 logL: -6.58e+03 KL: 5.12e+01 MMD: 4.61e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.69e+03 logL: -6.52e+03 KL: 5.63e+01 MMD: 4.64e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.59e+03 logL: -6.42e+03 KL: 6.03e+01 MMD: 4.42e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.53e+03 logL: -6.35e+03 KL: 6.32e+01 MMD: 4.65e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.30e+03 logL: -6.12e+03 KL: 6.48e+01 MMD: 4.57e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.30e+03 logL: -6.13e+03 KL: 6.49e+01 MMD: 4.42e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.24e+03 logL: -6.06e+03 KL: 6.45e+01 MMD: 4.64e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.17e+03 logL: -5.99e+03 KL: 6.33e+01 MMD: 4.48e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.20e+03 logL: -6.03e+03 KL: 6.27e+01 MMD: 4.57e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.97e+03 logL: -5.80e+03 KL: 6.25e+01 MMD: 4.27e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.98e+03 logL: -5.81e+03 KL: 6.25e+01 MMD: 4.55e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.95e+03 logL: -5.78e+03 KL: 6.25e+01 MMD: 4.28e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.95e+03 logL: -5.78e+03 KL: 6.25e+01 MMD: 4.21e+00\n",
      "Stopping\n",
      "====> Epoch: 101 VALIDATION Loss: 5.95e+03 logL: -5.78e+03 KL: 6.25e+01 MMD: 4.26e+00\n",
      "config: alpha = 0.0, lambda = 2.3; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.67e+04 logL: -1.67e+04 KL: 1.76e+01 MMD: 5.08e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.24e+04 logL: -1.24e+04 KL: 2.54e+01 MMD: 6.37e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.15e+04 logL: -1.14e+04 KL: 2.97e+01 MMD: 6.99e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.67e+03 logL: -9.63e+03 KL: 3.59e+01 MMD: 6.68e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.28e+03 logL: -8.23e+03 KL: 4.40e+01 MMD: 6.93e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.43e+03 logL: -7.37e+03 KL: 5.15e+01 MMD: 6.39e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 6.99e+03 logL: -6.93e+03 KL: 5.79e+01 MMD: 6.50e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.67e+03 logL: -6.60e+03 KL: 6.35e+01 MMD: 6.04e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.52e+03 logL: -6.45e+03 KL: 6.80e+01 MMD: 6.10e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.34e+03 logL: -6.26e+03 KL: 7.15e+01 MMD: 6.02e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.25e+03 logL: -6.17e+03 KL: 7.29e+01 MMD: 6.06e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.30e+03 logL: -6.22e+03 KL: 7.26e+01 MMD: 6.02e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.14e+03 logL: -6.06e+03 KL: 7.11e+01 MMD: 6.18e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.07e+03 logL: -6.00e+03 KL: 6.91e+01 MMD: 5.99e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.11e+03 logL: -6.03e+03 KL: 6.69e+01 MMD: 6.03e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.04e+03 logL: -5.96e+03 KL: 6.55e+01 MMD: 5.87e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.81e+03 logL: -5.74e+03 KL: 6.48e+01 MMD: 5.92e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.81e+03 logL: -5.74e+03 KL: 6.46e+01 MMD: 5.89e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.80e+03 logL: -5.72e+03 KL: 6.45e+01 MMD: 5.93e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.79e+03 logL: -5.72e+03 KL: 6.45e+01 MMD: 6.07e+00\n",
      "Stopping\n",
      "====> Epoch: 104 VALIDATION Loss: 5.79e+03 logL: -5.72e+03 KL: 6.45e+01 MMD: 5.76e+00\n",
      "config: alpha = 0.0, lambda = 8.4; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.76e+04 logL: -1.75e+04 KL: 1.48e+01 MMD: 5.25e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.28e+04 logL: -1.28e+04 KL: 2.24e+01 MMD: 5.88e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.15e+04 logL: -1.14e+04 KL: 2.75e+01 MMD: 6.26e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.38e+03 logL: -9.30e+03 KL: 3.32e+01 MMD: 6.12e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.20e+03 logL: -8.11e+03 KL: 3.99e+01 MMD: 5.88e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.45e+03 logL: -7.37e+03 KL: 4.63e+01 MMD: 5.39e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.07e+03 logL: -6.98e+03 KL: 5.24e+01 MMD: 5.11e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.59e+03 logL: -6.50e+03 KL: 5.77e+01 MMD: 4.74e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.47e+03 logL: -6.38e+03 KL: 6.23e+01 MMD: 4.64e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.31e+03 logL: -6.21e+03 KL: 6.58e+01 MMD: 4.59e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.24e+03 logL: -6.13e+03 KL: 6.80e+01 MMD: 4.61e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.14e+03 logL: -6.04e+03 KL: 6.85e+01 MMD: 4.23e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.13e+03 logL: -6.03e+03 KL: 6.78e+01 MMD: 4.41e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.19e+03 logL: -6.09e+03 KL: 6.67e+01 MMD: 4.60e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 5.95e+03 logL: -5.85e+03 KL: 6.50e+01 MMD: 4.21e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.98e+03 logL: -5.88e+03 KL: 6.34e+01 MMD: 4.44e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.77e+03 logL: -5.68e+03 KL: 6.31e+01 MMD: 4.39e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.78e+03 logL: -5.68e+03 KL: 6.30e+01 MMD: 4.35e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.76e+03 logL: -5.66e+03 KL: 6.30e+01 MMD: 4.64e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.76e+03 logL: -5.66e+03 KL: 6.30e+01 MMD: 4.14e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 5.75e+03 logL: -5.66e+03 KL: 6.30e+01 MMD: 4.33e+00\n",
      "Stopping\n",
      "====> Epoch: 106 VALIDATION Loss: 5.76e+03 logL: -5.66e+03 KL: 6.30e+01 MMD: 4.36e+00\n",
      "config: alpha = 0.0, lambda = 6.3; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.84e+04 logL: -1.84e+04 KL: 1.26e+01 MMD: 4.14e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.39e+04 logL: -1.38e+04 KL: 1.98e+01 MMD: 5.27e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.22e+04 logL: -1.22e+04 KL: 2.47e+01 MMD: 5.88e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.07e+04 logL: -1.07e+04 KL: 2.93e+01 MMD: 5.93e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.06e+03 logL: -8.99e+03 KL: 3.60e+01 MMD: 5.64e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 8.15e+03 logL: -8.07e+03 KL: 4.40e+01 MMD: 5.30e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.28e+03 logL: -7.20e+03 KL: 5.04e+01 MMD: 5.06e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.02e+03 logL: -6.94e+03 KL: 5.60e+01 MMD: 4.86e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.84e+03 logL: -6.75e+03 KL: 6.07e+01 MMD: 4.51e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.68e+03 logL: -6.59e+03 KL: 6.48e+01 MMD: 4.75e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.55e+03 logL: -6.46e+03 KL: 6.77e+01 MMD: 4.70e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.44e+03 logL: -6.35e+03 KL: 6.94e+01 MMD: 4.52e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.36e+03 logL: -6.26e+03 KL: 6.98e+01 MMD: 4.54e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.41e+03 logL: -6.32e+03 KL: 6.91e+01 MMD: 4.55e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.24e+03 logL: -6.14e+03 KL: 6.79e+01 MMD: 4.58e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.20e+03 logL: -6.11e+03 KL: 6.62e+01 MMD: 4.56e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.19e+03 logL: -6.10e+03 KL: 6.50e+01 MMD: 4.52e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.17e+03 logL: -6.08e+03 KL: 6.41e+01 MMD: 4.39e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.96e+03 logL: -5.87e+03 KL: 6.40e+01 MMD: 4.49e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.94e+03 logL: -5.85e+03 KL: 6.39e+01 MMD: 4.26e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 5.94e+03 logL: -5.85e+03 KL: 6.39e+01 MMD: 4.54e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.94e+03 logL: -5.85e+03 KL: 6.39e+01 MMD: 4.55e+00\n",
      "Stopping\n",
      "====> Epoch: 114 VALIDATION Loss: 5.94e+03 logL: -5.85e+03 KL: 6.39e+01 MMD: 4.46e+00\n",
      "config: alpha = 0.0, lambda = 27.8; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.80e+04 logL: -1.78e+04 KL: 1.27e+01 MMD: 4.65e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+04 logL: -1.43e+04 KL: 1.74e+01 MMD: 5.81e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.21e+04 logL: -1.19e+04 KL: 2.24e+01 MMD: 6.16e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.05e+04 logL: -1.03e+04 KL: 2.82e+01 MMD: 5.90e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.68e+03 logL: -8.49e+03 KL: 3.71e+01 MMD: 5.78e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.94e+03 logL: -7.75e+03 KL: 4.53e+01 MMD: 5.31e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.37e+03 logL: -7.19e+03 KL: 5.21e+01 MMD: 5.00e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.05e+03 logL: -6.86e+03 KL: 5.82e+01 MMD: 4.92e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.74e+03 logL: -6.54e+03 KL: 6.33e+01 MMD: 4.80e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.66e+03 logL: -6.46e+03 KL: 6.76e+01 MMD: 4.84e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.47e+03 logL: -6.27e+03 KL: 7.03e+01 MMD: 4.79e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.54e+03 logL: -6.34e+03 KL: 7.13e+01 MMD: 4.72e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.37e+03 logL: -6.18e+03 KL: 7.12e+01 MMD: 4.51e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.37e+03 logL: -6.18e+03 KL: 7.00e+01 MMD: 4.48e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.25e+03 logL: -6.06e+03 KL: 6.92e+01 MMD: 4.37e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.25e+03 logL: -6.06e+03 KL: 6.72e+01 MMD: 4.55e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.17e+03 logL: -5.98e+03 KL: 6.57e+01 MMD: 4.62e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.14e+03 logL: -5.95e+03 KL: 6.45e+01 MMD: 4.48e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.18e+03 logL: -6.00e+03 KL: 6.36e+01 MMD: 4.32e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.18e+03 logL: -5.99e+03 KL: 6.28e+01 MMD: 4.70e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 6.08e+03 logL: -5.90e+03 KL: 6.20e+01 MMD: 4.36e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.07e+03 logL: -5.89e+03 KL: 6.14e+01 MMD: 4.54e+00\n",
      "====> Epoch: 115 VALIDATION Loss: 5.89e+03 logL: -5.71e+03 KL: 6.13e+01 MMD: 4.34e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 5.89e+03 logL: -5.70e+03 KL: 6.13e+01 MMD: 4.67e+00\n",
      "====> Epoch: 125 VALIDATION Loss: 5.87e+03 logL: -5.69e+03 KL: 6.13e+01 MMD: 4.47e+00\n",
      "====> Epoch: 130 VALIDATION Loss: 5.88e+03 logL: -5.69e+03 KL: 6.13e+01 MMD: 4.57e+00\n",
      "Stopping\n",
      "====> Epoch: 134 VALIDATION Loss: 5.87e+03 logL: -5.69e+03 KL: 6.13e+01 MMD: 4.34e+00\n",
      "config: alpha = 0.0, lambda = 14.2; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.77e+04 logL: -1.76e+04 KL: 1.56e+01 MMD: 5.61e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.32e+04 logL: -1.31e+04 KL: 2.22e+01 MMD: 6.50e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.14e+04 logL: -1.13e+04 KL: 2.72e+01 MMD: 6.92e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.94e+03 logL: -9.82e+03 KL: 3.26e+01 MMD: 6.92e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.43e+03 logL: -8.31e+03 KL: 4.06e+01 MMD: 6.35e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.48e+03 logL: -7.36e+03 KL: 4.78e+01 MMD: 5.87e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.00e+03 logL: -6.87e+03 KL: 5.38e+01 MMD: 5.75e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.68e+03 logL: -6.54e+03 KL: 5.90e+01 MMD: 5.77e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.51e+03 logL: -6.37e+03 KL: 6.35e+01 MMD: 5.66e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.54e+03 logL: -6.40e+03 KL: 6.68e+01 MMD: 5.42e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.39e+03 logL: -6.25e+03 KL: 6.88e+01 MMD: 5.15e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.24e+03 logL: -6.10e+03 KL: 6.93e+01 MMD: 5.43e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.19e+03 logL: -6.05e+03 KL: 6.88e+01 MMD: 4.97e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.18e+03 logL: -6.05e+03 KL: 6.78e+01 MMD: 5.07e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.21e+03 logL: -6.07e+03 KL: 6.65e+01 MMD: 4.95e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.08e+03 logL: -5.96e+03 KL: 6.55e+01 MMD: 4.73e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.87e+03 logL: -5.74e+03 KL: 6.43e+01 MMD: 4.81e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.88e+03 logL: -5.75e+03 KL: 6.42e+01 MMD: 4.87e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.85e+03 logL: -5.73e+03 KL: 6.43e+01 MMD: 4.53e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.86e+03 logL: -5.73e+03 KL: 6.42e+01 MMD: 5.16e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 5.86e+03 logL: -5.73e+03 KL: 6.42e+01 MMD: 4.80e+00\n",
      "Stopping\n",
      "====> Epoch: 105 VALIDATION Loss: 5.86e+03 logL: -5.73e+03 KL: 6.42e+01 MMD: 4.80e+00\n",
      "Best Validation loss: 5757.527\n",
      "Best Parameters: [8.389687114068746]\n",
      "config: alpha = 0.0, lambda = 17.4; hidden layers with [256, 128], nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 5 VALIDATION Loss: 1.79e+04 logL: -1.78e+04 KL: 1.85e+01 MMD: 5.88e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.35e+04 logL: -1.33e+04 KL: 2.79e+01 MMD: 7.31e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.23e+04 logL: -1.22e+04 KL: 3.41e+01 MMD: 7.55e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.02e+04 logL: -1.00e+04 KL: 4.10e+01 MMD: 7.55e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.74e+03 logL: -8.57e+03 KL: 4.88e+01 MMD: 7.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.89e+03 logL: -7.72e+03 KL: 5.70e+01 MMD: 6.80e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.23e+03 logL: -7.06e+03 KL: 6.40e+01 MMD: 6.58e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.09e+03 logL: -6.92e+03 KL: 7.05e+01 MMD: 6.21e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.89e+03 logL: -6.72e+03 KL: 7.62e+01 MMD: 5.94e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.57e+03 logL: -6.39e+03 KL: 8.07e+01 MMD: 5.74e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.45e+03 logL: -6.27e+03 KL: 8.39e+01 MMD: 5.69e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.36e+03 logL: -6.18e+03 KL: 8.49e+01 MMD: 5.69e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.31e+03 logL: -6.14e+03 KL: 8.44e+01 MMD: 5.49e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.30e+03 logL: -6.13e+03 KL: 8.28e+01 MMD: 5.71e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.20e+03 logL: -6.03e+03 KL: 8.11e+01 MMD: 5.58e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.14e+03 logL: -5.97e+03 KL: 7.90e+01 MMD: 5.73e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.12e+03 logL: -5.95e+03 KL: 7.75e+01 MMD: 5.35e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.93e+03 logL: -5.76e+03 KL: 7.67e+01 MMD: 5.31e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.91e+03 logL: -5.74e+03 KL: 7.67e+01 MMD: 5.30e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.91e+03 logL: -5.74e+03 KL: 7.67e+01 MMD: 5.55e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 5.91e+03 logL: -5.74e+03 KL: 7.67e+01 MMD: 5.49e+00\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 5.91e+03 logL: -5.74e+03 KL: 7.67e+01 MMD: 5.39e+00\n",
      "config: alpha = 0.0, lambda = 57.1; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.75e+04 logL: -1.71e+04 KL: 1.67e+01 MMD: 6.12e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.37e+04 logL: -1.32e+04 KL: 2.38e+01 MMD: 7.45e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.20e+04 logL: -1.15e+04 KL: 2.94e+01 MMD: 8.08e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.01e+04 logL: -9.58e+03 KL: 3.63e+01 MMD: 7.96e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.75e+03 logL: -8.30e+03 KL: 4.49e+01 MMD: 7.21e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.74e+03 logL: -7.30e+03 KL: 5.37e+01 MMD: 6.79e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.20e+03 logL: -6.79e+03 KL: 6.15e+01 MMD: 6.28e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.94e+03 logL: -6.53e+03 KL: 6.84e+01 MMD: 6.09e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.81e+03 logL: -6.40e+03 KL: 7.45e+01 MMD: 6.01e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.65e+03 logL: -6.25e+03 KL: 7.95e+01 MMD: 5.61e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.53e+03 logL: -6.14e+03 KL: 8.28e+01 MMD: 5.45e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.49e+03 logL: -6.10e+03 KL: 8.47e+01 MMD: 5.35e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.43e+03 logL: -6.05e+03 KL: 8.49e+01 MMD: 5.19e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.56e+03 logL: -6.18e+03 KL: 8.39e+01 MMD: 5.20e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.37e+03 logL: -6.00e+03 KL: 8.29e+01 MMD: 5.18e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.36e+03 logL: -5.98e+03 KL: 8.16e+01 MMD: 5.27e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.33e+03 logL: -5.96e+03 KL: 7.98e+01 MMD: 5.17e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.28e+03 logL: -5.91e+03 KL: 7.83e+01 MMD: 5.23e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 6.07e+03 logL: -5.70e+03 KL: 7.76e+01 MMD: 5.29e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 6.08e+03 logL: -5.71e+03 KL: 7.75e+01 MMD: 5.24e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 6.07e+03 logL: -5.70e+03 KL: 7.74e+01 MMD: 5.27e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 6.08e+03 logL: -5.70e+03 KL: 7.73e+01 MMD: 5.53e+00\n",
      "====> Epoch: 115 VALIDATION Loss: 6.05e+03 logL: -5.68e+03 KL: 7.73e+01 MMD: 5.10e+00\n",
      "====> Epoch: 120 VALIDATION Loss: 6.07e+03 logL: -5.68e+03 KL: 7.73e+01 MMD: 5.48e+00\n",
      "Stopping\n",
      "====> Epoch: 121 VALIDATION Loss: 6.06e+03 logL: -5.68e+03 KL: 7.73e+01 MMD: 5.38e+00\n",
      "config: alpha = 0.0, lambda = 7.8; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.77e+04 logL: -1.76e+04 KL: 1.58e+01 MMD: 4.77e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.45e+04 logL: -1.44e+04 KL: 2.22e+01 MMD: 6.62e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.20e+04 logL: -1.19e+04 KL: 2.81e+01 MMD: 7.59e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.05e+04 logL: -1.04e+04 KL: 3.48e+01 MMD: 7.90e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.42e+03 logL: -8.32e+03 KL: 4.56e+01 MMD: 7.60e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.37e+03 logL: -7.27e+03 KL: 5.47e+01 MMD: 7.14e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 6.99e+03 logL: -6.88e+03 KL: 6.24e+01 MMD: 6.99e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.62e+03 logL: -6.50e+03 KL: 6.92e+01 MMD: 6.81e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.42e+03 logL: -6.30e+03 KL: 7.51e+01 MMD: 6.55e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.44e+03 logL: -6.32e+03 KL: 7.99e+01 MMD: 6.56e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.24e+03 logL: -6.12e+03 KL: 8.29e+01 MMD: 6.19e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.21e+03 logL: -6.09e+03 KL: 8.44e+01 MMD: 6.48e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.14e+03 logL: -6.01e+03 KL: 8.39e+01 MMD: 6.40e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.13e+03 logL: -6.01e+03 KL: 8.29e+01 MMD: 6.14e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.02e+03 logL: -5.89e+03 KL: 8.12e+01 MMD: 6.42e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.00e+03 logL: -5.88e+03 KL: 7.96e+01 MMD: 6.13e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.92e+03 logL: -5.80e+03 KL: 7.83e+01 MMD: 6.02e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.96e+03 logL: -5.85e+03 KL: 7.70e+01 MMD: 6.14e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.87e+03 logL: -5.75e+03 KL: 7.59e+01 MMD: 6.29e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.96e+03 logL: -5.84e+03 KL: 7.52e+01 MMD: 5.91e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 5.70e+03 logL: -5.58e+03 KL: 7.50e+01 MMD: 6.19e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.69e+03 logL: -5.58e+03 KL: 7.50e+01 MMD: 5.94e+00\n",
      "Stopping\n",
      "====> Epoch: 113 VALIDATION Loss: 5.70e+03 logL: -5.59e+03 KL: 7.50e+01 MMD: 5.85e+00\n",
      "config: alpha = 0.0, lambda = 31.4; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.74e+04 logL: -1.72e+04 KL: 1.96e+01 MMD: 6.84e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.26e+04 logL: -1.23e+04 KL: 2.80e+01 MMD: 7.85e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.14e+04 logL: -1.12e+04 KL: 3.26e+01 MMD: 8.09e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.68e+03 logL: -9.40e+03 KL: 3.94e+01 MMD: 7.91e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.34e+03 logL: -8.07e+03 KL: 4.85e+01 MMD: 7.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.43e+03 logL: -7.17e+03 KL: 5.74e+01 MMD: 6.67e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 6.98e+03 logL: -6.72e+03 KL: 6.56e+01 MMD: 6.43e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.71e+03 logL: -6.45e+03 KL: 7.29e+01 MMD: 5.89e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.53e+03 logL: -6.27e+03 KL: 7.92e+01 MMD: 5.88e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.40e+03 logL: -6.14e+03 KL: 8.36e+01 MMD: 5.87e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.28e+03 logL: -6.03e+03 KL: 8.63e+01 MMD: 5.58e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.24e+03 logL: -5.98e+03 KL: 8.70e+01 MMD: 5.72e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.18e+03 logL: -5.92e+03 KL: 8.69e+01 MMD: 5.59e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.06e+03 logL: -5.81e+03 KL: 8.55e+01 MMD: 5.44e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 5.87e+03 logL: -5.62e+03 KL: 8.45e+01 MMD: 5.44e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.86e+03 logL: -5.61e+03 KL: 8.43e+01 MMD: 5.41e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.85e+03 logL: -5.60e+03 KL: 8.42e+01 MMD: 5.45e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.84e+03 logL: -5.60e+03 KL: 8.42e+01 MMD: 5.31e+00\n",
      "Stopping\n",
      "====> Epoch: 94 VALIDATION Loss: 5.84e+03 logL: -5.59e+03 KL: 8.42e+01 MMD: 5.55e+00\n",
      "config: alpha = 0.0, lambda = 4.5; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.76e+04 logL: -1.76e+04 KL: 1.50e+01 MMD: 5.47e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.32e+04 logL: -1.31e+04 KL: 2.36e+01 MMD: 7.19e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.19e+04 logL: -1.19e+04 KL: 2.96e+01 MMD: 7.79e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.98e+03 logL: -9.91e+03 KL: 3.68e+01 MMD: 7.53e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.51e+03 logL: -8.44e+03 KL: 4.59e+01 MMD: 7.43e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.48e+03 logL: -7.41e+03 KL: 5.44e+01 MMD: 6.88e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 6.93e+03 logL: -6.84e+03 KL: 6.21e+01 MMD: 6.78e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.79e+03 logL: -6.70e+03 KL: 6.87e+01 MMD: 6.28e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.56e+03 logL: -6.46e+03 KL: 7.44e+01 MMD: 6.12e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.37e+03 logL: -6.27e+03 KL: 7.89e+01 MMD: 6.30e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.27e+03 logL: -6.16e+03 KL: 8.18e+01 MMD: 6.27e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.29e+03 logL: -6.19e+03 KL: 8.36e+01 MMD: 6.19e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.25e+03 logL: -6.15e+03 KL: 8.38e+01 MMD: 5.73e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.09e+03 logL: -5.99e+03 KL: 8.28e+01 MMD: 6.06e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.06e+03 logL: -5.96e+03 KL: 8.13e+01 MMD: 6.06e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.06e+03 logL: -5.96e+03 KL: 7.95e+01 MMD: 5.90e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.95e+03 logL: -5.85e+03 KL: 7.81e+01 MMD: 5.95e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.95e+03 logL: -5.85e+03 KL: 7.69e+01 MMD: 6.08e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.89e+03 logL: -5.80e+03 KL: 7.59e+01 MMD: 5.86e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.92e+03 logL: -5.83e+03 KL: 7.55e+01 MMD: 5.92e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 5.86e+03 logL: -5.76e+03 KL: 7.46e+01 MMD: 5.94e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.83e+03 logL: -5.74e+03 KL: 7.43e+01 MMD: 5.68e+00\n",
      "====> Epoch: 115 VALIDATION Loss: 5.84e+03 logL: -5.74e+03 KL: 7.42e+01 MMD: 5.97e+00\n",
      "Stopping\n",
      "====> Epoch: 116 VALIDATION Loss: 5.85e+03 logL: -5.75e+03 KL: 7.42e+01 MMD: 6.24e+00\n",
      "config: alpha = 0.0, lambda = 9.6; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.73e+04 logL: -1.73e+04 KL: 1.94e+01 MMD: 5.99e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.33e+04 logL: -1.32e+04 KL: 2.84e+01 MMD: 7.33e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.16e+04 logL: -1.15e+04 KL: 3.36e+01 MMD: 7.81e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.93e+03 logL: -9.82e+03 KL: 3.99e+01 MMD: 7.80e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.29e+03 logL: -8.18e+03 KL: 4.88e+01 MMD: 7.54e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.47e+03 logL: -7.35e+03 KL: 5.75e+01 MMD: 7.02e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 6.94e+03 logL: -6.81e+03 KL: 6.48e+01 MMD: 7.04e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.61e+03 logL: -6.48e+03 KL: 7.14e+01 MMD: 6.55e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.49e+03 logL: -6.36e+03 KL: 7.73e+01 MMD: 6.40e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.29e+03 logL: -6.16e+03 KL: 8.17e+01 MMD: 6.37e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.19e+03 logL: -6.05e+03 KL: 8.43e+01 MMD: 6.22e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.17e+03 logL: -6.04e+03 KL: 8.51e+01 MMD: 6.08e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.09e+03 logL: -5.95e+03 KL: 8.45e+01 MMD: 5.99e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.08e+03 logL: -5.95e+03 KL: 8.28e+01 MMD: 5.85e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.03e+03 logL: -5.90e+03 KL: 8.09e+01 MMD: 5.86e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.04e+03 logL: -5.91e+03 KL: 7.91e+01 MMD: 6.22e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.75e+03 logL: -5.62e+03 KL: 7.80e+01 MMD: 5.93e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.74e+03 logL: -5.61e+03 KL: 7.79e+01 MMD: 5.85e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.74e+03 logL: -5.61e+03 KL: 7.78e+01 MMD: 5.95e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.73e+03 logL: -5.60e+03 KL: 7.77e+01 MMD: 6.00e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 5.72e+03 logL: -5.59e+03 KL: 7.76e+01 MMD: 5.87e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.72e+03 logL: -5.59e+03 KL: 7.76e+01 MMD: 5.72e+00\n",
      "====> Epoch: 115 VALIDATION Loss: 5.72e+03 logL: -5.59e+03 KL: 7.76e+01 MMD: 5.81e+00\n",
      "Stopping\n",
      "====> Epoch: 119 VALIDATION Loss: 5.71e+03 logL: -5.59e+03 KL: 7.76e+01 MMD: 5.79e+00\n",
      "config: alpha = 0.0, lambda = 18.8; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.84e+04 logL: -1.83e+04 KL: 1.69e+01 MMD: 4.90e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.44e+04 logL: -1.42e+04 KL: 2.47e+01 MMD: 6.57e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.24e+04 logL: -1.22e+04 KL: 3.04e+01 MMD: 7.12e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 1.06e+04 logL: -1.04e+04 KL: 3.65e+01 MMD: 7.26e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 9.04e+03 logL: -8.87e+03 KL: 4.49e+01 MMD: 7.04e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.98e+03 logL: -7.81e+03 KL: 5.35e+01 MMD: 6.83e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 7.52e+03 logL: -7.34e+03 KL: 6.16e+01 MMD: 6.57e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 7.01e+03 logL: -6.83e+03 KL: 6.92e+01 MMD: 6.41e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.78e+03 logL: -6.59e+03 KL: 7.52e+01 MMD: 6.28e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.69e+03 logL: -6.51e+03 KL: 8.05e+01 MMD: 5.91e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.57e+03 logL: -6.38e+03 KL: 8.38e+01 MMD: 6.06e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.41e+03 logL: -6.22e+03 KL: 8.53e+01 MMD: 5.87e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.39e+03 logL: -6.21e+03 KL: 8.52e+01 MMD: 5.64e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.34e+03 logL: -6.15e+03 KL: 8.36e+01 MMD: 5.87e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.49e+03 logL: -6.30e+03 KL: 8.12e+01 MMD: 5.87e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.06e+03 logL: -5.88e+03 KL: 8.00e+01 MMD: 5.69e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 6.06e+03 logL: -5.88e+03 KL: 7.98e+01 MMD: 5.88e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 6.05e+03 logL: -5.87e+03 KL: 7.97e+01 MMD: 5.88e+00\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 6.06e+03 logL: -5.88e+03 KL: 7.97e+01 MMD: 5.72e+00\n",
      "config: alpha = 0.0, lambda = 3.6; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.73e+04 logL: -1.73e+04 KL: 1.84e+01 MMD: 6.84e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.28e+04 logL: -1.28e+04 KL: 2.66e+01 MMD: 8.34e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.12e+04 logL: -1.11e+04 KL: 3.32e+01 MMD: 8.38e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.23e+03 logL: -9.17e+03 KL: 4.10e+01 MMD: 8.07e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 7.67e+03 logL: -7.60e+03 KL: 5.00e+01 MMD: 7.20e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.12e+03 logL: -7.04e+03 KL: 5.81e+01 MMD: 7.00e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 6.67e+03 logL: -6.59e+03 KL: 6.52e+01 MMD: 6.89e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.44e+03 logL: -6.35e+03 KL: 7.14e+01 MMD: 6.83e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.30e+03 logL: -6.20e+03 KL: 7.67e+01 MMD: 6.50e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.25e+03 logL: -6.16e+03 KL: 8.06e+01 MMD: 6.34e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.13e+03 logL: -6.03e+03 KL: 8.28e+01 MMD: 6.39e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.02e+03 logL: -5.92e+03 KL: 8.35e+01 MMD: 5.90e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 5.98e+03 logL: -5.88e+03 KL: 8.26e+01 MMD: 6.49e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.02e+03 logL: -5.93e+03 KL: 8.15e+01 MMD: 5.90e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 5.75e+03 logL: -5.66e+03 KL: 8.06e+01 MMD: 5.99e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.74e+03 logL: -5.64e+03 KL: 8.05e+01 MMD: 6.02e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.74e+03 logL: -5.65e+03 KL: 8.04e+01 MMD: 6.02e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.72e+03 logL: -5.63e+03 KL: 8.03e+01 MMD: 6.31e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.73e+03 logL: -5.63e+03 KL: 8.03e+01 MMD: 6.12e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.72e+03 logL: -5.63e+03 KL: 8.03e+01 MMD: 6.47e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 5.73e+03 logL: -5.63e+03 KL: 8.03e+01 MMD: 6.09e+00\n",
      "Stopping\n",
      "====> Epoch: 107 VALIDATION Loss: 5.72e+03 logL: -5.63e+03 KL: 8.03e+01 MMD: 6.63e+00\n",
      "config: alpha = 0.0, lambda = 4.5; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.72e+04 logL: -1.72e+04 KL: 1.61e+01 MMD: 5.13e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.31e+04 logL: -1.30e+04 KL: 2.37e+01 MMD: 6.73e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.17e+04 logL: -1.16e+04 KL: 2.93e+01 MMD: 7.41e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.81e+03 logL: -9.75e+03 KL: 3.62e+01 MMD: 7.41e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.22e+03 logL: -8.15e+03 KL: 4.56e+01 MMD: 7.32e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.47e+03 logL: -7.39e+03 KL: 5.46e+01 MMD: 7.01e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 6.91e+03 logL: -6.83e+03 KL: 6.24e+01 MMD: 6.61e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.65e+03 logL: -6.56e+03 KL: 6.91e+01 MMD: 6.52e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.54e+03 logL: -6.45e+03 KL: 7.52e+01 MMD: 6.45e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.31e+03 logL: -6.20e+03 KL: 7.98e+01 MMD: 6.73e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.22e+03 logL: -6.12e+03 KL: 8.27e+01 MMD: 6.45e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.14e+03 logL: -6.03e+03 KL: 8.38e+01 MMD: 5.97e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.07e+03 logL: -5.96e+03 KL: 8.31e+01 MMD: 6.24e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 6.14e+03 logL: -6.03e+03 KL: 8.16e+01 MMD: 6.14e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 6.02e+03 logL: -5.92e+03 KL: 7.96e+01 MMD: 5.87e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 6.06e+03 logL: -5.96e+03 KL: 7.83e+01 MMD: 6.06e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.78e+03 logL: -5.68e+03 KL: 7.74e+01 MMD: 6.26e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.78e+03 logL: -5.68e+03 KL: 7.74e+01 MMD: 6.11e+00\n",
      "====> Epoch: 95 VALIDATION Loss: 5.77e+03 logL: -5.68e+03 KL: 7.73e+01 MMD: 6.33e+00\n",
      "====> Epoch: 100 VALIDATION Loss: 5.77e+03 logL: -5.67e+03 KL: 7.72e+01 MMD: 6.20e+00\n",
      "====> Epoch: 105 VALIDATION Loss: 5.75e+03 logL: -5.66e+03 KL: 7.72e+01 MMD: 6.09e+00\n",
      "====> Epoch: 110 VALIDATION Loss: 5.75e+03 logL: -5.66e+03 KL: 7.72e+01 MMD: 5.85e+00\n",
      "Stopping\n",
      "====> Epoch: 111 VALIDATION Loss: 5.75e+03 logL: -5.65e+03 KL: 7.72e+01 MMD: 6.53e+00\n",
      "config: alpha = 0.0, lambda = 3.8; hidden layers with [256, 128], nodes\n",
      "====> Epoch: 5 VALIDATION Loss: 1.71e+04 logL: -1.71e+04 KL: 1.78e+01 MMD: 6.06e+00\n",
      "====> Epoch: 10 VALIDATION Loss: 1.32e+04 logL: -1.32e+04 KL: 2.61e+01 MMD: 7.32e+00\n",
      "====> Epoch: 15 VALIDATION Loss: 1.15e+04 logL: -1.14e+04 KL: 3.21e+01 MMD: 7.68e+00\n",
      "====> Epoch: 20 VALIDATION Loss: 9.87e+03 logL: -9.81e+03 KL: 3.96e+01 MMD: 7.71e+00\n",
      "====> Epoch: 25 VALIDATION Loss: 8.10e+03 logL: -8.03e+03 KL: 4.95e+01 MMD: 7.07e+00\n",
      "====> Epoch: 30 VALIDATION Loss: 7.48e+03 logL: -7.40e+03 KL: 5.86e+01 MMD: 6.99e+00\n",
      "====> Epoch: 35 VALIDATION Loss: 6.85e+03 logL: -6.76e+03 KL: 6.65e+01 MMD: 6.74e+00\n",
      "====> Epoch: 40 VALIDATION Loss: 6.59e+03 logL: -6.50e+03 KL: 7.32e+01 MMD: 6.18e+00\n",
      "====> Epoch: 45 VALIDATION Loss: 6.33e+03 logL: -6.23e+03 KL: 7.92e+01 MMD: 6.51e+00\n",
      "====> Epoch: 50 VALIDATION Loss: 6.28e+03 logL: -6.18e+03 KL: 8.36e+01 MMD: 6.23e+00\n",
      "====> Epoch: 55 VALIDATION Loss: 6.17e+03 logL: -6.07e+03 KL: 8.58e+01 MMD: 6.40e+00\n",
      "====> Epoch: 60 VALIDATION Loss: 6.19e+03 logL: -6.08e+03 KL: 8.63e+01 MMD: 6.28e+00\n",
      "====> Epoch: 65 VALIDATION Loss: 6.01e+03 logL: -5.91e+03 KL: 8.57e+01 MMD: 6.46e+00\n",
      "====> Epoch: 70 VALIDATION Loss: 5.99e+03 logL: -5.89e+03 KL: 8.39e+01 MMD: 6.36e+00\n",
      "====> Epoch: 75 VALIDATION Loss: 5.90e+03 logL: -5.80e+03 KL: 8.19e+01 MMD: 6.22e+00\n",
      "====> Epoch: 80 VALIDATION Loss: 5.90e+03 logL: -5.81e+03 KL: 8.00e+01 MMD: 5.83e+00\n",
      "====> Epoch: 85 VALIDATION Loss: 5.84e+03 logL: -5.75e+03 KL: 7.85e+01 MMD: 6.23e+00\n",
      "====> Epoch: 90 VALIDATION Loss: 5.89e+03 logL: -5.80e+03 KL: 7.70e+01 MMD: 5.96e+00\n",
      "Stopping\n",
      "====> Epoch: 91 VALIDATION Loss: 5.94e+03 logL: -5.84e+03 KL: 7.67e+01 MMD: 5.87e+00\n",
      "Best Validation loss: 5701.087\n",
      "Best Parameters: [7.823182203193062]\n"
     ]
    }
   ],
   "source": [
    "n_calls = 10\n",
    "opti_results = []\n",
    "\n",
    "# perform optimization on each of the latent dimension values\n",
    "for nc in ncode_vals:\n",
    "    # ncode is that variable above (global variable)\n",
    "    ncode = nc\n",
    "\n",
    "    # run optimization\n",
    "    result = gp_minimize(evaluate_model, search_space, n_calls=n_calls)\n",
    "    opti_results.append(result)\n",
    "    # summarizing finding:\n",
    "    print('Best Validation loss: %.3f' % (result.fun))\n",
    "    print('Best Parameters: %s' % (result.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eca2c952-e229-4d14-aeb2-8f0f8db67508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation loss: 5701.087\n",
      "Best Parameters: [7.823182203193062]\n"
     ]
    }
   ],
   "source": [
    "print('Best Validation loss: %.3f' % (result.fun))\n",
    "print('Best Parameters: %s' % (result.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11844215-f6e9-4b27-8ecf-7abfab14edba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of latent dimensions:  3\n",
      "Best Validation loss: 8519.655\n",
      "Best Parameters: [5.043310624057961]\n",
      "Number of latent dimensions:  6\n",
      "Best Validation loss: 6327.255\n",
      "Best Parameters: [6.059668283296925]\n",
      "Number of latent dimensions:  12\n",
      "Best Validation loss: 5757.527\n",
      "Best Parameters: [8.389687114068746]\n",
      "Number of latent dimensions:  15\n",
      "Best Validation loss: 5701.087\n",
      "Best Parameters: [7.823182203193062]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ncode_vals)):\n",
    "    print('Number of latent dimensions: ', ncode_vals[i])\n",
    "    # summarizing finding:\n",
    "    print('Best Validation loss: %.3f' % (opti_results[i].fun))\n",
    "    print('Best Parameters: %s' % (opti_results[i].x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359b03f-cf68-42be-a25b-f8612b71668e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801e748-b4a7-4821-a36d-bf62d3a6490e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe2121-53f5-475c-a4cf-d1259d7196a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
